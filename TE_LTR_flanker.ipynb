{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## What is this script good for ?\n",
    "Consensus files of TE sequences often need a tedious pre-processing : LTR transposable elements are splitted between their intern and LTR parts. If you want to use those TE sequences for an alignment, you have to restore the real sequence of these TEs by flanking the intern part with the LTR part (like this : LTR|Intern|LTR ).\n",
    "\n",
    "The goal of this script is to simplify this task and to provide a correctly reconstructed new fasta file.\n",
    "\n",
    "## How to use it ?\n",
    "\n",
    "This script need two files : a classic **consensus fasta file** (an example being the Dfam families.fa file present in the folder) and a **dictionnary in tsv format** providing correspondance between intern and LTR parts.\n",
    "\n",
    "1) Generating the dictionnary\n",
    "\n",
    "You can choose to write the dictionnary yourself (TE_name, TE_intern_part, TE_LTR_part, tab_separated):\n",
    "\n",
    "dictionnary example :  \n",
    "`Copia  Copia_I    Copia_LTR`  \n",
    "`Roo   Roo-I_DM   Roo-LTR_DM`\n",
    "\n",
    "OR generate it automatically using 'generate_dictionnary.py' script and the default list of suffixes (\"standard_suffixes.tsv\"):\n",
    "\n",
    "`./generate_dictionnary.py -f families.fa --suffix standard_suffixes.tsv -o families.dictionnary.tsv`\n",
    "\n",
    "You can also specify a custom list of suffixes in a tsv file, with the first line being the list of intern suffixes, and the second line being the list of LTR suffixes, separated with tabs.\n",
    "\n",
    "ex :  \n",
    "`_I -I_DM`  \n",
    "`_LTR   -LTR_DM`\n",
    "\n",
    "**It is advised to have a look at the generated dictionnary and manually cure it if needed.**\n",
    "\n",
    "2) Get the new consensus fasta file using this command:\n",
    "\n",
    "`./generate_new_consensus_fasta.py -f families.fa -d families.dictionnary -o new_families.fa`\n",
    "\n",
    "Notes :\n",
    "\n",
    "All TE sequences that are present in the fasta must be represented in the dictionnary.\n",
    "TODO : Maybe add the option to directly output TE sequences that are not written in the dictionnary.\n",
    "This can be easily implemented and tested by removing a random TE from the dictionnary...\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# from warnings import warn\n",
    "import logging\n",
    "from Bio import SeqIO\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "Dfam_consensus_fasta = \"example_data/families.fa\"\n",
    "suffix_file = \"standard_suffixes.tsv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR:root:These TE names ends with a recognized suffix, but couldn't be matched. You might want to manually cure some of them in the dictionnary :\n",
      "HMSBEAGLE_I, TOM_I\n",
      "Stalker3_LTR, Gypsy6A_LTR, DMTOM1_LTR, DM412B_LTR, Gypsy12A_LTR\n",
      "\n",
      "Invader6\tInvader6_I\tInvader6_LTR\n",
      "BATUMI\tBATUMI_I\tBATUMI_LTR\n",
      "MICROPIA\tMICROPIA_I\tMICROPIA_LTR\n",
      "NOMAD\tNOMAD_I\tNOMAD_LTR\n",
      "ZAM\tZAM_I\tZAM_LTR\n",
      "Gypsy9\tGypsy9_I\tGypsy9_LTR\n",
      "ROVER\tROVER-I_DM\tROVER-LTR_DM\n",
      "STALKER4\tSTALKER4_I\tSTALKER4_LTR\n",
      "MDG1\tMDG1_I\tMDG1_LTR\n",
      "Copia2\tCopia2_I\tCopia2_LTR_DM\n",
      "Gypsy5\tGypsy5_I\tGypsy5_LTR\n",
      "MDG3\tMDG3_I\tMDG3_LTR\n",
      "DIVER\tDIVER_I\tDIVER_LTR\n",
      "GTWIN\tGTWIN_I\tGTWIN_LTR\n",
      "BURDOCK\tBURDOCK_I\tBURDOCK_LTR\n",
      "Gypsy7\tGypsy7_I\tGypsy7_LTR\n",
      "IDEFIX\tIDEFIX_I\tIDEFIX_LTR\n",
      "Invader1\tInvader1_I\tInvader1_LTR\n",
      "Chouto\tChouto_I\tChouto_LTR\n",
      "BLASTOPIA\tBLASTOPIA_I\tBLASTOPIA_LTR\n",
      "Bica\tBica_I\tBica_LTR\n",
      "Invader2\tInvader2_I\tInvader2_LTR\n",
      "QUASIMODO2\tQUASIMODO2-I_DM\tQUASIMODO2-LTR_DM\n",
      "QUASIMODO\tQUASIMODO_I\tQUASIMODO_LTR\n",
      "Stalker2\tStalker2_I\tStalker2_LTR\n",
      "Gypsy3\tGypsy3_I\tGypsy3_LTR\n",
      "DM1731\tDM1731_I\tDM1731_LTR\n",
      "Invader4\tInvader4_I\tInvader4_LTR\n",
      "Gypsy10\tGypsy10_I\tGypsy10_LTR\n",
      "ACCORD2\tACCORD2_I\tACCORD2_LTR\n",
      "Copia\tCopia_I\tCopia_LTR\n",
      "BEL\tBEL_I\tBEL_LTR\n",
      "Invader5\tInvader5_I\tInvader5_LTR\n",
      "MAX\tMAX_I\tMAX_LTR\n",
      "Gypsy6\tGypsy6_I\tGypsy6_LTR\n",
      "Invader3\tInvader3_I\tInvader3_LTR\n",
      "DM297\tDM297_I\tDM297_LTR\n",
      "Gypsy2\tGypsy2-I_DM\tGypsy2-LTR_DM\n",
      "TIRANT\tTIRANT_I\tTIRANT_LTR\n",
      "Gypsy8\tGypsy8_I\tGypsy8_LTR\n",
      "Copia1\tCopia1-I_DM\tCopia1-LTR_DM\n",
      "ROOA\tROOA_I\tROOA_LTR\n",
      "BLOOD\tBLOOD_I\tBLOOD_LTR\n",
      "Gypsy\tGypsy_I\tGypsy_LTR\n",
      "Gypsy1\tGypsy1-I_DM\tGypsy1-LTR_DM\n",
      "ACCORD\tACCORD_I\tACCORD_LTR\n",
      "Chimpo\tChimpo_I\tChimpo_LTR\n",
      "DIVER2\tDIVER2_I\tDIVER2_LTR\n",
      "DM176\tDM176_I\tDM176_LTR\n",
      "NINJA\tNINJA_I\tNINJA_LTR\n",
      "Gypsy4\tGypsy4_I\tGypsy4_LTR\n",
      "TABOR\tTABOR_I\tTABOR_LTR\n",
      "ROO\tROO_I\tROO_LTR\n",
      "Gypsy11\tGypsy11_I\tGypsy11_LTR\n",
      "TRANSPAC\tTRANSPAC_I\tTRANSPAC_LTR\n",
      "Gypsy12\tGypsy12_I\tGypsy12_LTR\n",
      "FROGGER\tFROGGER_I\tFROGGER_LTR\n",
      "HMSBEAGLE_I\n",
      "TOM_I\n",
      "Stalker3_LTR\n",
      "Gypsy6A_LTR\n",
      "DMTOM1_LTR\n",
      "DM412B_LTR\n",
      "Gypsy12A_LTR\n",
      "XDMR_DM\n",
      "XDMR\n",
      "5S_DM\n",
      "ALA_DM\n",
      "ARS406_DM\n",
      "Baggins1\n",
      "BARI1\n",
      "BARI_DM\n",
      "BS\n",
      "BS2\n",
      "BS3_DM\n",
      "BS4_DM\n",
      "CIRCE\n",
      "DM412\n",
      "DMCR1A\n",
      "DMLTR5\n",
      "DMRP1\n",
      "DMRPR\n",
      "DMRT1A\n",
      "DMRT1B\n",
      "DMRT1C\n",
      "DMSAT6\n",
      "DNAREP1_DM\n",
      "DOC\n",
      "DOC2_DM\n",
      "DOC3_DM\n",
      "DOC4_DM\n",
      "DOC5_DM\n",
      "DOC6_DM\n",
      "FB4_DM\n",
      "FTZ_DM\n",
      "FUSHI_DM\n",
      "FW2_DM\n",
      "FW3_DM\n",
      "G2_DM\n",
      "G3_DM\n",
      "G4_DM\n",
      "G5A_DM\n",
      "G5_DM\n",
      "G6_DM\n",
      "G7_DM\n",
      "G_DM\n",
      "HELENA_RT\n",
      "Helitron1_DM\n",
      "HETA\n",
      "HETRP_DM\n",
      "HOBO\n",
      "I_DM\n",
      "IVK_DM\n",
      "Jockey2\n",
      "LINEJ1_DM\n",
      "LOOPER1_DM\n",
      "M4DM\n",
      "Mariner2_DM\n",
      "MINOS\n",
      "NOF_FB\n",
      "NTS_DM\n",
      "PLACW_DM\n",
      "POGO\n",
      "POGON1\n",
      "PROTOP\n",
      "PROTOP_A\n",
      "PROTOP_B\n",
      "R1-2_DM\n",
      "R1_DM\n",
      "R2_DM\n",
      "RSP\n",
      "SAR2_DM\n",
      "SAR_DM\n",
      "S_DM\n",
      "TAHRE\n",
      "TART-A\n",
      "TART_B1\n",
      "TC1-2_DM\n",
      "TC1_DM\n",
      "TLD2\n",
      "TRANSIB1\n",
      "TRANSIB2\n",
      "TRANSIB3\n",
      "TRANSIB4\n",
      "Transib5\n",
      "Transib-N1_DM\n",
      "S2_DM\n"
     ]
    }
   ],
   "source": [
    "## Dictionnary generator\n",
    "\n",
    "def get_TE_ID(record):\n",
    "    return record.split()[-1]\n",
    "\n",
    "def generate_matching_pairs_from_suffixes(consensus_fasta, suffix_file):\n",
    "    with open(suffix_file, 'r') as input:\n",
    "        I_suffix_list = input.readline().split()\n",
    "        LTR_suffix_list = input.readline().split()\n",
    "    I_suffix_set = set(I_suffix_list)\n",
    "    LTR_suffix_set = set(LTR_suffix_list)\n",
    "\n",
    "    def is_LTR_part(name):\n",
    "        for suffix in LTR_suffix_set:\n",
    "            if name.endswith(suffix):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_I_part(name):\n",
    "        for suffix in I_suffix_set:\n",
    "            if name.endswith(suffix):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def remove_suffix(seq_ID):\n",
    "        suffix_list = list(LTR_suffix_set) + list(I_suffix_set)\n",
    "        for suffix in suffix_list:\n",
    "            if seq_ID.endswith(suffix):\n",
    "                return seq_ID[:-len(suffix)]\n",
    "        return seq_ID\n",
    "\n",
    "    I_part_dict = {}\n",
    "    LTR_part_dict = {}\n",
    "    not_splitted_TE = []\n",
    "    detected_conflicts = []\n",
    "\n",
    "    records = list(SeqIO.parse(consensus_fasta, \"fasta\"))\n",
    "    for record in records:\n",
    "        TE_ID = get_TE_ID(record.description)\n",
    "        TE_name = remove_suffix(TE_ID)\n",
    "        if is_LTR_part(TE_ID):\n",
    "            if TE_name in LTR_part_dict :\n",
    "                detected_conflicts.append(TE_name)\n",
    "                continue\n",
    "            LTR_part_dict[TE_name] = TE_ID\n",
    "        elif is_I_part(TE_ID):\n",
    "            if TE_name in I_part_dict :\n",
    "                detected_conflicts.append(TE_name)\n",
    "                continue\n",
    "            I_part_dict[TE_name] = TE_ID\n",
    "        else:\n",
    "            not_splitted_TE.append(TE_ID)\n",
    "\n",
    "    if detected_conflicts :\n",
    "        logging.warning(\"Detected conflict, following TEs can be matched with several suffixes : \" + ', '.join(set(detected_conflicts)) + \"\\nYou should manually set it in the dictionnary.\")\n",
    "    \n",
    "    matching_TE_parts = set(I_part_dict.keys()).intersection(set(LTR_part_dict.keys()))\n",
    "    unmatched_I_list = [I_part_dict[x] for x in set(I_part_dict.keys()).difference(set(LTR_part_dict.keys()))]\n",
    "    unmatched_LTR_list = [LTR_part_dict[x] for x in set(LTR_part_dict.keys()).difference(set(I_part_dict.keys()))]\n",
    "    if len(unmatched_I_list + unmatched_LTR_list) > 0 :\n",
    "        logging.warning(\"These TE names ends with a recognized suffix, but couldn't be matched. You might want to manually cure some of them in the dictionnary :\\n\" + ', '.join(unmatched_I_list) + \"\\n\" + ', '.join(unmatched_LTR_list) + \"\\n\")\n",
    "    TE_dictionnary = \"\"\n",
    "    for TE_name in matching_TE_parts:\n",
    "        TE_dictionnary += \"\\t\".join([TE_name, I_part_dict[TE_name], LTR_part_dict[TE_name]]) + \"\\n\"\n",
    "    TE_dictionnary += '\\n'.join(unmatched_I_list + unmatched_LTR_list + not_splitted_TE)\n",
    "    return TE_dictionnary\n",
    "\n",
    "print(generate_matching_pairs_from_suffixes(Dfam_consensus_fasta, suffix_file))\n"
   ]
  },
  {
   "source": [
    "## Manually inspecting and curing the dictionnary..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TE_dictionnary_file = \"example_data/families.dictionnary.curated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating new fasta with flanked intern parts using dictionnary\n",
    "\n",
    "def import_dictionnary(TE_dictionnary_file):\n",
    "    TE_dict = dict()\n",
    "    with open(TE_dictionnary_file, 'r') as dictionnary:\n",
    "        for line in dictionnary :\n",
    "            if len(line.split()) == 3:\n",
    "                TE_name, I_part, LTR_part = line.split()\n",
    "                TE_dict[TE_name] = [I_part, LTR_part]\n",
    "            elif len(line.split()) == 1:\n",
    "                TE_name = line.strip()\n",
    "                TE_dict[TE_name] = []\n",
    "            else :\n",
    "                raise ValueError(\"Invalid dictionnary line : \\n\" + line + \"\\nEach line should contains either 3 columns corresponding to the TE_name, TE_intern_part, TE_LTR_part OR only contains 1 column corresponding to the TE_name.\")\n",
    "    return TE_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generating_LTR_flanked_fasta_file(consensus_fasta, TE_dictionnary_file):\n",
    "    TE_dict = import_dictionnary(TE_dictionnary_file)\n",
    "    I_dict = {v[0] : k for k, v in TE_dict.items() if v}\n",
    "    LTR_dict = {v[1] : k for k, v in TE_dict.items() if v}\n",
    "\n",
    "    record_iterator = SeqIO.parse(consensus_fasta, \"fasta\")\n",
    "    reconstructed_records = dict()\n",
    "    new_records = []\n",
    "    for record in record_iterator:\n",
    "        TE_ID = get_TE_ID(record.description)\n",
    "        if TE_ID in TE_dict: # If TE_ID directly in keys, then it means its a non_splitted TE\n",
    "            record.id = TE_ID\n",
    "            record.description = \"\"\n",
    "            new_records.append(record)\n",
    "        elif TE_ID in I_dict:\n",
    "            if I_dict[TE_ID] not in reconstructed_records :\n",
    "                reconstructed_records[I_dict[TE_ID]] = [record, None]\n",
    "            else :\n",
    "                reconstructed_records[I_dict[TE_ID]][0] = record\n",
    "        elif TE_ID in LTR_dict:\n",
    "            if LTR_dict[TE_ID] not in reconstructed_records :\n",
    "                reconstructed_records[LTR_dict[TE_ID]] = [None, record]\n",
    "            else :\n",
    "                reconstructed_records[LTR_dict[TE_ID]][1] = record\n",
    "        else:\n",
    "            raise ValueError\n",
    "    for merged_TE_ID, TE_parts_list in reconstructed_records.items() :\n",
    "        I_part_record, LTR_part_record = TE_parts_list\n",
    "        new_record = I_part_record\n",
    "        new_record.seq = LTR_part_record.seq + I_part_record.seq + LTR_part_record.seq\n",
    "\n",
    "        # merged_TE_ID = get_TE_hierarchy_from_embl_file(merged_TE_ID, embl_hierarchy_dict)\n",
    "\n",
    "        new_record.id = merged_TE_ID\n",
    "        new_record.description = \"\"\n",
    "        new_records.append(new_record)\n",
    "    SeqIO.write(new_records, \"my_example.faa\", \"fasta\")\n",
    "\n",
    "generating_LTR_flanked_fasta_file(Dfam_consensus_fasta, TE_dictionnary_file)"
   ]
  },
  {
   "source": [
    "## Next cell if just custom code to add hierarchy of TE in TE_name"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "embl_file = \"example_data/families.embl\"\n",
    "TE_dict = import_dictionnary(TE_dictionnary_file)\n",
    "\n",
    "class EmblRecord:\n",
    "    def __init__(self, acc, name, hierarchy):\n",
    "        self.acc = acc\n",
    "        self.name = name\n",
    "        self.hierarchy = hierarchy\n",
    "\n",
    "def embl_parser(embl_file):\n",
    "    custom_embl_dict = {}\n",
    "    with open(embl_file,'r') as input:\n",
    "        for line in input:\n",
    "            if line.startswith(\"CC\"):\n",
    "                if line.startswith(\"CC        Type:\"):\n",
    "                    TE_type = line.split(\":\")[1]\n",
    "                if line.startswith(\"CC        SubType:\"):\n",
    "                    TE_subtype = line.split(\":\")[1]\n",
    "                continue\n",
    "            if line.startswith(\"AC\"):\n",
    "                acc = line.split()[-1].replace(\";\", \"\")\n",
    "            if line.startswith(\"NM\"):\n",
    "                name = line.split()[-1]\n",
    "            if line.startswith(\"//\"):\n",
    "                if not TE_subtype.strip():\n",
    "                    TE_subtype = \"Unknown\"\n",
    "                hierarchy = TE_type.strip() + \"/\" + TE_subtype.strip()\n",
    "\n",
    "                new_record = EmblRecord(acc, name, hierarchy)\n",
    "                custom_embl_dict[name] = new_record\n",
    "    return custom_embl_dict\n",
    "\n",
    "def build_TE_hierarchy_dict_from_embl_file(TE_dict, embl_file):\n",
    "    embl_dict = embl_parser(embl_file)\n",
    "    TE_hierarchy_dict = dict()\n",
    "    for TE, TE_parts in TE_dict.items():\n",
    "        if TE_parts:\n",
    "            record = embl_dict[TE_parts[0]]\n",
    "        elif TE in embl_dict:\n",
    "            record = embl_dict[TE]\n",
    "        else:\n",
    "            logging.warning(\"TE `\" + TE + \"` could not be match in EMBL file. No hierarchy will be added.\")\n",
    "            continue\n",
    "\n",
    "        TE_hierarchy_dict[TE] = TE + '#'+ record.hierarchy\n",
    "    return TE_hierarchy_dict\n",
    "\n",
    "TE_hierarchy_dict = build_TE_hierarchy_dict_from_embl_file(TE_dict, embl_file)\n",
    "\n",
    "def generating_LTR_flanked_fasta_file_with_hierarchy(consensus_fasta, TE_dictionnary_file, TE_hierarchy_dict):\n",
    "    TE_dict = import_dictionnary(TE_dictionnary_file)\n",
    "    I_dict = {v[0] : k for k, v in TE_dict.items() if v}\n",
    "    LTR_dict = {v[1] : k for k, v in TE_dict.items() if v}\n",
    "\n",
    "    record_iterator = SeqIO.parse(consensus_fasta, \"fasta\")\n",
    "    reconstructed_records = dict()\n",
    "    new_records = []\n",
    "    for record in record_iterator:\n",
    "        TE_ID = record.description.split()[-1]\n",
    "        # print(TE_ID)\n",
    "        # print(TE_hierarchy_dict[LTR_dict[TE_ID]])\n",
    "        # break\n",
    "        if TE_ID in TE_dict: # If TE_ID directly in keys, then it means its a non_splitted TE\n",
    "            record.id = TE_hierarchy_dict[TE_ID]\n",
    "            record.description = \"\"\n",
    "            new_records.append(record)\n",
    "        elif TE_ID in I_dict:\n",
    "            if I_dict[TE_ID] not in reconstructed_records :\n",
    "                reconstructed_records[I_dict[TE_ID]] = [record, None]\n",
    "            else :\n",
    "                reconstructed_records[I_dict[TE_ID]][0] = record\n",
    "        elif TE_ID in LTR_dict:\n",
    "            if LTR_dict[TE_ID] not in reconstructed_records :\n",
    "                reconstructed_records[LTR_dict[TE_ID]] = [None, record]\n",
    "            else :\n",
    "                reconstructed_records[LTR_dict[TE_ID]][1] = record\n",
    "        else:\n",
    "            raise ValueError\n",
    "    for merged_TE_ID, TE_parts_list in reconstructed_records.items() :\n",
    "        I_part_record, LTR_part_record = TE_parts_list\n",
    "        new_record = I_part_record\n",
    "        new_record.seq = LTR_part_record.seq + I_part_record.seq + LTR_part_record.seq\n",
    "        new_record.id = TE_hierarchy_dict[merged_TE_ID]\n",
    "        new_record.description = \"\"\n",
    "        new_records.append(new_record)\n",
    "    SeqIO.write(new_records, \"example_data/families.flanked_LTR.hierarchy.fa\", \"fasta\")\n",
    "\n",
    "generating_LTR_flanked_fasta_file_with_hierarchy(Dfam_consensus_fasta, TE_dictionnary_file, TE_hierarchy_dict)"
   ]
  },
  {
   "source": [
    "### Checking if it went well..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID: DF0001619.1\nName: DF0001619.1\nDescription: DF0001619.1 Gypsy2_I\nNumber of features: 0\nSeq('ggcgcccaaccagtggtatttgacagtgcgttagtcattacccacgaacaaaaa...ggt')\nID: DF0001620.1\nName: DF0001620.1\nDescription: DF0001620.1 Gypsy2-I_DM\nNumber of features: 0\nSeq('gggaccagcgaataacgcgtacgacagacaaaattctaagtcgcgaagcaaaat...tat')\nID: DF0001621.1\nName: DF0001621.1\nDescription: DF0001621.1 Gypsy2_LTR\nNumber of features: 0\nSeq('agttaaccaaagctaacgtcgctcccacgggcgtatgaaatacgacaacancgg...att')\nID: DF0001622.1\nName: DF0001622.1\nDescription: DF0001622.1 Gypsy2-LTR_DM\nNumber of features: 0\nSeq('tgccctacaagtnattgtcccacatcttgtntttcaacatcgtttctgggtaaa...aca')\n"
     ]
    }
   ],
   "source": [
    "record_iterator = SeqIO.parse(Dfam_consensus_fasta, \"fasta\")\n",
    "\n",
    "for record in record_iterator:\n",
    "    if record.description == \"DF0001619.1 Gypsy2_I\":\n",
    "        print(record)\n",
    "    if record.description == \"DF0001620.1 Gypsy2-I_DM\":\n",
    "        print(record)\n",
    "    if record.description == \"DF0001621.1 Gypsy2_LTR\":\n",
    "        print(record)\n",
    "    if record.description == \"DF0001622.1 Gypsy2-LTR_DM\":\n",
    "        print(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "agttaccgacccatcggtaccatacaccacccctccctctaaaccaccacgcctacacaagtagaagacatcgaaccgggaagctttgcgatacaaagtcgcaacataaacatcaacgacgagtcagccgccgacatccgcccaaaaagctgacaccgcattcttttcgctcagacagggcaacgcatacaattccatatacatacttataaacatactcatactttctgctgtgtcagatactttatttctaagaactttaacattgtaacacacacatacatattcactgttagcttatttaagaccaagaataaagacgacgacagtcgagntcgagcagcaagcacttgtggacgtatctaatctccgatcaaaattctctggagacagccgaggctacattctggatcgatacaactcctctctcccgctgtattanaatacctccgcgaagctccccggaggtaac\ntggcgcagccgggaaactggaatggaaaatactctattaaaccttttattagttctattgtaagtagttgtggaaaaagagtgagaatgaagtgcagaaatgtctaaaagtgattacaacaaaagtcctactacaatacataaaccgccttaacaaacatacaaaacacanatataaangnaaaaaaaaaaaaaaaaanaaaaannaanacccnaaacttaaaaatgccgtaatcgtgaaacatgatatgtgttgtacttgcgtggnatcaatcgctgataatcactgccgaaatttattaaggccaagtaccacatcantactctcacgtatacatacatatatatgcnnnacaattaaaacaacatacacacacacaaatatttcaaatgcaaaaaaanangaangaatgtagtgtacctgcgtggcatcaatcgctgataaaccactgccgaaatattaaaggcccggtactacatcacaaaacanatatatatgcaacaaaaatatacacaacaaaaccatatatacanacgtgtaagagtgacgtgtaatgtacttgcgtgaaatcaatcgctgataatcactgccgaagtttantaaggccaagtaccacgtcattactnanatgcgtacatacatatatatgcaaaacaattaaaacaacatacacacacacgaatattttaaatgcaaaaaaaananaaaaagangaaatgtagtgtacctgcgtggcatcgatcgctgataaaccactgccgaaatattaaaggcccggtactacatcacaaaacacgtatatatacaacaaaaatatacacaacaaaaccatatatacaaacgtgtaagagtgacgtgtaatgtacttgtgtgaaatcaatcgctgataatcactgccgaagcttagtaaggccaagtaccacatcattactnncatgtgtacatatatatatgcaaaccaccaaaacaaatacatatacacatacaaanattccaaatannnnnncgaataatattatatgaacggtgaagcgtatggtttctaaggctggatacaaaaccacaaaatcacatataaattgcacatcttaataaagaaaagagaaaaaataataataaacaaaagaaattttttttgaaacaagcacccatactctcactcttttaatacaaataaagtattcaaattatacatacatacactaataccactatattacagaaattaacgcacaagaaaacacacacactatccaacaacaaacaagtaattaagagtcattaagtacattgtaaactgtatatttttctcttaaatgtcaaagaaattaacacaaactataaaacaaacaactcgttccgtgttaggatcacacacaattccaaaaagagttacacgatcagtttctaaaacaaacaccctccccgtaattagagaaagtacccccttaccgccccttcaacctataaatatggattcgggcaacgcctccgtgggtaattccgccnccttaacacctaccgtcagtggctttagcagtattgctacggcacttagtgccaccgatattttagccttcgttaaagaacttccgactttcgatggtactccaggccaactcgacaaatatataactagcgttgaggaaataatcatgctcattaggggtaccgaccaaacaccgtacggacttctgacactcagggcaattaggaataaaatagttggaagagcagacgaagctctaaacctagccaacaccaaacttatatgggacgatatcaaaagtaacctactacgtttatactctagcaagaaaagcgaagctaccctcttaagcgagctccaatctctcccggataacctaaccctagggcaattgttcttcggcttatcgaggattaggagccaacttatatccactacttccaatagtggacagtcggccacaatcatcgaagccaagaaaacactatatgacgaagtctgtctaaatgccttcatttcaagaattagagaaccacttaaaacagtcatcagattgaaagaccccaagactatcgaaacagcttacgagctatgtcaaagagaaagggctcgttaccagaacagaaacccataccccccaacacaaaacaacaccgaacgacgaactaacaattacaataacaataacaacaacaatcacagagacaacaacgaccgcaacaacgtaactcatcttacacccaaaaccactcaaaccattattcaaaccccaantcccaatatcgtcaatcaaacaacggcaacagaactagtaacccgtttaaagacaataaaacaaactatgggctacacaacatagaagaagaaaaactcacccaacactgccctaccaacctaaattttcaggcacccgcctcaggaacccaacaggatacataaatcctaccacacatgcaacatccctcccatacataactctaaacctccaacaaaaattccctttgtcatttcttatcgatacaggatccaataactccttcattgacccagaatctgcaaaccaactagagtgcacaattcnaccaacatccacttcaattacaacagcattaaatagtttcaaaattgaggaaaaggcaatattcccaatgccacccgagttcaaaaccaaaggtcaaattaccctacttaaattcaaatttcactcttatttcaatggcctcataggaatggacctattatcacacctagaagcaaaaatagacctagtaaacttacgactagtaacttcaaagtctacactcccaatattcctgtacactaaccaggcctcaaaaatttttaacatccccgcctacagtaaagtcatcttaccactaccggtaaagactaatcatggggaattctattgttgtactacacaactaaataatgagttatcgttgtcagaaggactatataaatcaaacaataatattgccaacgtcgaaatctctaaccaatccgactcagataaactattatacctagaataccccctagaaaccatcccatacaataaaaacgaccatatcgagctctttaatatatcagctacacctcttaataacgataccccccaagccccattacanatcctaacagaacacctcaatccagaagaaaaaacagctttaacaaccctatgtaaacaatttcgcgacatattctacaacccagaaacaccattaactttcaccaacaaaatcacacactccatccccaccatagacaacactcctatccacgcaaaatcctacagatacccttttgtccataaaacagaagtcaaaaaacaaatcgaatccatgttagaccaananatnattagatctagccactccccttggagcgccccggtntgggtggtcccaaaaaaactagacggnacagggaacaggaaatggcgactngtaatagactacnggaaactnaacganaaaaccatttcggacagataccccatcccaaacataaacgacatattagatagcataggcaaagcnaaatatttctcaacgctcgacctagctagcggttttcatcaaatcgagatgaacccaaaagatatcgccaaaacagcctttacagtcgaagggggtcactacgaattcgtacgaatgcccttcggcttaaaaaacgcaccggcnacctttcaacgggtnatggacagcgttcttggcgatctnaacggcancatttgcttagtctatctngacgatattataattttctcgccttccctacaagaacacctatcggacntaaaaatggtattcgaaaaactnagagcggcaaactttaaactacaacctnaaaaatcggaatttctaaggaaagagatngaatncctnggccacatagtcacacaagacggaatnaaaccaaacccgaacaaaataantgcgatcaaaaaatttccttgccccaccaacagaagagcnattaaatcntttctnggattactgggntattataggaagtttatnagagacttcgcangaataacgaagcccntgacnaaacaattgaaagggaaaagacaagttacnacagacgaagantttgtagangcattcgaanngtgcaaaactcttctntncaatgacccnatnctcatacacccagacttcganaaaccattcattctnactacggacgctagcaatttcgcgttaggngcngtactatctcaaggcnctttacgaaacgacagacccgtatctttcgccagcagaacnctntccgacaccgaagtnaactattcnaccgtagaaaaagaaatgttggcaataatntgggcagtaaaatattttagaccatatatttatggcgnaaaattcacnattgttacagaccacaagccactaatntggctnatgaatttcaaagaaccnaactcnaaactagtccgntggagactccaactcntggaatacgatttcgaaatagtttacaagaaaggttcgcaaaacgtngtcgcagacgcnttaagtagagcggacccaaatttaaaccacaacgaaacactgactgttaagccttgccccacatccgaaaaaccnatnaacgaatttaacacgcaactcatactagaaatagatacaaatacgtctcgccaaactacaacaccntttaaacaaaaaattaggaagaaatattcacagccttgcttcgatttcgataacattgctaaaatcttgaaaggaaccctaaaacctaacaggatttgcgcattcttggcggacgataataattccgcattaatcgaaaaagcattctnaacgtattttgcacacaanaaacactttaaaattatcagatgcaaancacttctccacgaaatcgtaggaaaccccgaacaaaacaaattcatncaggaatatcacactaacaacaatcacagaggnatagacgaaacattcctncacctcaaangagaaacctatttccccaatatgaaaaacaaaatctctgaattaattaanaattgcgaaacctgtcnaaaactcaaatacgacagacaaccgcaaaanatagtatttgaaaccccagaaaccccatcgaaacccctcgacataatacacacggacatntatactattaacganaattttaacctgacaatnatagacaagttctcgaaattcgcggcngnctacncnatcccaantagggacggtatcaatngcaccaaagcaatnagaaattttttcagtcaattcggaataccnaaaaaactaatncgcgaccaaggngccgaattttgcaacgacntntttcgaaagttttgctcncaatataatatagntttacacgtcacgtcnttccaacaatcttcgagtaattctccagtagaangnttncactcctctctgacagaaatntacagaataatactagacanaaggaaaaaacacaaactacctacngaccacgaagaaatnttgtcagaaacnntaatnacatataacaacgcaatccactccgccacnaaacncaccccnttcgaactttttaacggnagaacccatttattcganaaaacaatnacacccgataacgagcacgactatttaaataaactaaatacgtttcaagacaaactataccccgaaataaaagaaaaattgtccacaaacgcccaacaaaggacagaaaagctaaacgcaagcagagtagaaccaacgacngtacaaccnaacagcacaattttcagaaaagaaaacaggagaaataaattaacaccacggttttccttacacagaacagcaaangacaaaggnnnaacnttngtaaccacaagaaatcaaaaaatncacaaatcaaaaattaggaaaatatccaaacctccaaatgacttaagcctttccacctgcattccagatcttgccatggggcataccaatctatcttcatccacaacttcaatagcaccaacctcctagcaaaagtgccgctagggaaaacnctcgtgataggaaactataaaaaaattagccacataatcgatctgtccgaatacaccaactgtattgaaaaattataccacaccatcgataccctaagacaagatgaaacgctcatcgactctatatcaatactaaangctaaactngcccaanctcgaagnaaaatagacgcactaacaccctcggnaagacacaaacggggncttattaacggattaggnagtntcgtcaaagtcgtcaccggcaacatggacgccaatgatgcaaagaatatagaaacagaaattaaccacttaaaaagccagtccaccactatcncagataacttcgaaatacagaactcgttcaatgatgaaattcaactacggttcgaaaacttaacaagacacattaacaatgaacagaattcgattaaaaacttcttcgaaaacactcaaaatacaatttacacaaaantatataacaacgaagaagaaataaagaaactacaatatataaataggcttaactataatatagatttattacttagccacctaagcgacattatagaaagtacactgcttgccaaaattaatgtnatnccaaaactcatnttagataagacagaaataaccaaaatcaaacaaatttttaaaacncaaaactacacaataaaatccgagcaacacatttataacctgttaaaaatgaacacactcaattaccaagacaaaataattttcagtatcaaaatncctatttttctaaatnntaactacgaaatggcaagattaattccacttccaataaattccacacaatttgtaatagcacctaagtacttaatatataataacaaaagtaatagtatgttttcaactatgtataaatgccctgtaatagaagaacaattcgtctgcgaaatcgactccatcaataatcttaaaaataatacttgcctgggacaccttatccagaataagaccagctactgcgacataaaggaaacgggactcacgaccgatgtgttcgaaccggaaaaaggcttcatatttgtatttaacgggaacaacctcccaatcatctcctccaancagaccataactaatatcaatggatcagctataataaagtataacaattgcacattacaaatcaatgaaataaactacgacaacacggcggtatcaacagaagagcaccccgacttcttcctaccaccaatgcggaaactaataaaaaatgccactatcaacatactcaccttggaaagacttcacctggatacactcacaacatccaataagctactggtcgtcgccgcaggaaactctcgacactcgacaaccttgtacattctcttcaccgtatccctagtcgccgtaatactcacctggacacttcgnagggacacccacgtcttccataccgggcccgaccacattcttccaatcgtcgctccaccaattcctccgtntatggccgtcgctccaaactggggggggagg\n"
     ]
    }
   ],
   "source": [
    "record_iterator = SeqIO.parse(Dfam_consensus_fasta, \"fasta\")\n",
    "\n",
    "for record in record_iterator:\n",
    "    if record.description == \"DF0001525.1 ZAM_LTR\":\n",
    "        print(record.seq)\n",
    "    if record.description == \"DF0001526.2 ZAM_I\":\n",
    "        print(record.seq)"
   ]
  }
 ]
}