{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## What is it good for ?\n",
    "Consensus files of TE sequences often need a tedious pre-processing : LTR transposable elements are splitted between their intern and LTR parts. If you want to use those TE sequences for an alignment, you have to restore the real sequence of these TEs by flanking the intern part with the LTR part (like this : LTR|Intern|LTR ).\n",
    "\n",
    "The goal of this script is to simplify this task and to provide a correctly reconstructed new fasta file.\n",
    "\n",
    "## How to use it ?\n",
    "\n",
    "This script need two files : a classic **consensus fasta file** (an example being the Dfam families.fa file present in the folder) and a **dictionnary in tsv format** providing correspondance between intern and LTR parts.\n",
    "\n",
    "1) Generating the dictionnary\n",
    "\n",
    "You can choose to write the dictionnary yourself (TE_name, TE_intern_part, TE_LTR_part, tab_separated):\n",
    "\n",
    "dictionnary example :  \n",
    "`Copia  Copia_I    Copia_LTR`  \n",
    "`Roo   Roo-I_DM   Roo-LTR_DM`\n",
    "\n",
    "OR generate it automatically using 'generate_dictionnary.py' script and the default list of suffixes (\"standard_suffixes.tsv\"):\n",
    "\n",
    "`generate_dictionnary.py families.fa standard_suffixes.tsv > families.dictionnary.tsv`\n",
    "\n",
    "You can also specify a custom list of suffixes in a tsv file, with the first line being the list of intern suffixes, and the second line being the list of LTR suffixes, separated with tabs.\n",
    "\n",
    "ex :  \n",
    "`_I -I_DM`  \n",
    "`_LTR   -LTR_DM`\n",
    "\n",
    "**It is advised to have a look at the generated dictionnary and manually cure it if needed.**\n",
    "\n",
    "2) Get the new consensus fasta file using this command:\n",
    "\n",
    "`TE_LTR_flanker.py families.fa standard_suffixes.tsv > new_families.fa`\n",
    "\n",
    "Notes :\n",
    "\n",
    "TE sequences that are not present in the dictionnary are written as such in the output.  \n",
    "The name used to describe the whole TE in the new fasta will correspond to the name of the intern part. (TODO : maybe give the option of providing a third column with TE names ?)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'_I', '-I_DM'}\n{'-LTR_DM', '_LTR_DM', '_LTR'}\noij\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "Dfam_consensus_fasta = \"families.fa\"\n",
    "suffix_file = \"standard_suffixes.tsv\"\n",
    "\n",
    "def generate_matching_pairs_from_suffixes(consensus_fasta, suffix_file):\n",
    "    with open(suffix_file, 'r') as input:\n",
    "        I_suffix_list = input.readline().split()\n",
    "        LTR_suffix_list = input.readline().split()\n",
    "    I_suffix_set = set(I_suffix_list)\n",
    "    LTR_suffix_set = set(LTR_suffix_list)\n",
    "    print(I_suffix_set)\n",
    "    print(LTR_suffix_set)\n",
    "\n",
    "    def is_LTR_part(name):\n",
    "        for suffix in LTR_suffix_set:\n",
    "            if name.endswith(suffix):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_I_part(name):\n",
    "        for suffix in I_suffix_set:\n",
    "            if name.endswith(suffix):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def remove_suffix(seq_ID):\n",
    "        suffix_list = list(LTR_suffix_set) + list(I_suffix_set)\n",
    "        for suffix in suffix_list:\n",
    "            if seq_ID.endswith(suffix):\n",
    "                return seq_ID[:-len(suffix)]\n",
    "        return seq_ID\n",
    "\n",
    "    I_part_dict = {}\n",
    "    LTR_part_dict = {}\n",
    "    not_splitted_TE = []\n",
    "\n",
    "    with open(consensus_fasta, 'r') as input:\n",
    "        for line in input:\n",
    "            if line.startswith('>'):\n",
    "                seq_ID = line.split()[-1].replace('>', '')\n",
    "                TE_name = remove_suffix(seq_ID)\n",
    "                if is_LTR_part(seq_ID):\n",
    "                    LTR_part_dict[TE_name] = seq_ID\n",
    "                elif is_I_part(seq_ID):\n",
    "                    I_part_dict[TE_name] = seq_ID\n",
    "                else:\n",
    "                    not_splitted_TE.append(seq_ID)\n",
    "    \n",
    "    matching_TE_parts = set(I_part_dict.keys()).intersection(set(LTR_part_dict.keys()))\n",
    "    # unmatched_TE_parts = set(I_part_dict.keys()).symmetric_difference(set(LTR_part_dict.keys()))\n",
    "    unmatched_I_list = [I_part_dict[x] for x in set(I_part_dict.keys()).difference(set(LTR_part_dict.keys()))]\n",
    "    unmatched_LTR_list = [LTR_part_dict[x] for x in set(LTR_part_dict.keys()).difference(set(I_part_dict.keys()))]\n",
    "    # print(unmatched_I_list)\n",
    "    # print(unmatched_LTR_list)\n",
    "\n",
    "    if len(unmatched_I_list + unmatched_LTR_list) > 0 :\n",
    "        warnings.simplefilter(\"default\")\n",
    "        print(\"oij\")\n",
    "\n",
    "        \n",
    "        # These TE names ends with a recognized suffix, but couldn't be match. You might want to manually cure them in the dictionnary :\\n\" + str(unmatched_I_list) + \"\\n\" + str(unmatched_LTR_list))\n",
    "    # print(matching_TE_parts)\n",
    "    TE_dictionnary = \"\"\n",
    "    # for TE_name in matching_TE_parts:\n",
    "    #     print(TE_name, I_part_dict[TE_name], LTR_part_dict[TE_name])\n",
    "    # for TE_name in unmatched_TE_parts:\n",
    "    #     if TE_name in I_part_dict:\n",
    "    #         print(I_part_dict[TE_name])\n",
    "    #     else :\n",
    "    #         print(LTR_part_dict[TE_name])\n",
    "    # for TE_name in not_splitted_TE:\n",
    "    #     print(TE_name)\n",
    "    # print(len(matching_TE_parts))\n",
    "    # print(len(unmatched_TE_parts))\n",
    "    # print(len(not_splitted_TE))\n",
    "generate_matching_pairs_from_suffixes(Dfam_consensus_fasta, suffix_file)\n",
    "\n",
    "# def match_I_and_LTR(consensus_fasta):\n",
    "#     with open(consensus_fasta, 'r') as consensus:\n",
    "        # for \n"
   ]
  }
 ]
}