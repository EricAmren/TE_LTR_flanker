{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## What is it good for ?\n",
    "Consensus files of TE sequences often need a tedious pre-processing : LTR transposable elements are splitted between their intern and LTR parts. If you want to use those TE sequences for an alignment, you have to restore the real sequence of these TEs by flanking the intern part with the LTR part (like this : LTR|Intern|LTR ).\n",
    "\n",
    "The goal of this script is to simplify this task and to provide a correctly reconstructed new fasta file.\n",
    "\n",
    "## How to use it ?\n",
    "\n",
    "This script need two files : a classic **consensus fasta file** (an example being the Dfam families.fa file present in the folder) and a **dictionnary in tsv format** providing correspondance between intern and LTR parts.\n",
    "\n",
    "1) Generating the dictionnary\n",
    "\n",
    "You can choose to write the dictionnary yourself (TE_name, TE_intern_part, TE_LTR_part, tab_separated):\n",
    "\n",
    "dictionnary example :  \n",
    "`Copia  Copia_I    Copia_LTR`  \n",
    "`Roo   Roo-I_DM   Roo-LTR_DM`\n",
    "\n",
    "OR generate it automatically using 'generate_dictionnary.py' script and the default list of suffixes (\"standard_suffixes.tsv\"):\n",
    "\n",
    "`generate_dictionnary.py families.fa standard_suffixes.tsv > families.dictionnary.tsv`\n",
    "\n",
    "You can also specify a custom list of suffixes in a tsv file, with the first line being the list of intern suffixes, and the second line being the list of LTR suffixes, separated with tabs.\n",
    "\n",
    "ex :  \n",
    "`_I -I_DM`  \n",
    "`_LTR   -LTR_DM`\n",
    "\n",
    "**It is advised to have a look at the generated dictionnary and manually cure it if needed.**\n",
    "\n",
    "2) Get the new consensus fasta file using this command:\n",
    "\n",
    "`TE_LTR_flanker.py families.fa standard_suffixes.tsv > new_families.fa`\n",
    "\n",
    "Notes :\n",
    "\n",
    "TE sequences that are not present in the dictionnary are written as such in the output.  \n",
    "The name used to describe the whole TE in the new fasta will correspond to the name of the intern part. (TODO : maybe give the option of providing a third column with TE names ?)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# from warnings import warn\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "Dfam_consensus_fasta = \"families.fa\"\n",
    "suffix_file = \"standard_suffixes.tsv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR:root:These TE names ends with a recognized suffix, but couldn't be matched. You might want to manually cure some of them in the dictionnary :\n",
      "HMSBEAGLE_I, TOM_I\n",
      "Stalker3_LTR, Gypsy6A_LTR, DMTOM1_LTR, DM412B_LTR, Gypsy12A_LTR\n",
      "\n",
      "Invader6\tInvader6_I\tInvader6_LTR\n",
      "BATUMI\tBATUMI_I\tBATUMI_LTR\n",
      "MICROPIA\tMICROPIA_I\tMICROPIA_LTR\n",
      "NOMAD\tNOMAD_I\tNOMAD_LTR\n",
      "ZAM\tZAM_I\tZAM_LTR\n",
      "Gypsy9\tGypsy9_I\tGypsy9_LTR\n",
      "ROVER\tROVER-I_DM\tROVER-LTR_DM\n",
      "STALKER4\tSTALKER4_I\tSTALKER4_LTR\n",
      "MDG1\tMDG1_I\tMDG1_LTR\n",
      "Copia2\tCopia2_I\tCopia2_LTR_DM\n",
      "Gypsy5\tGypsy5_I\tGypsy5_LTR\n",
      "MDG3\tMDG3_I\tMDG3_LTR\n",
      "DIVER\tDIVER_I\tDIVER_LTR\n",
      "GTWIN\tGTWIN_I\tGTWIN_LTR\n",
      "BURDOCK\tBURDOCK_I\tBURDOCK_LTR\n",
      "Gypsy7\tGypsy7_I\tGypsy7_LTR\n",
      "IDEFIX\tIDEFIX_I\tIDEFIX_LTR\n",
      "Invader1\tInvader1_I\tInvader1_LTR\n",
      "Chouto\tChouto_I\tChouto_LTR\n",
      "BLASTOPIA\tBLASTOPIA_I\tBLASTOPIA_LTR\n",
      "Bica\tBica_I\tBica_LTR\n",
      "Invader2\tInvader2_I\tInvader2_LTR\n",
      "QUASIMODO2\tQUASIMODO2-I_DM\tQUASIMODO2-LTR_DM\n",
      "QUASIMODO\tQUASIMODO_I\tQUASIMODO_LTR\n",
      "Stalker2\tStalker2_I\tStalker2_LTR\n",
      "Gypsy3\tGypsy3_I\tGypsy3_LTR\n",
      "DM1731\tDM1731_I\tDM1731_LTR\n",
      "Invader4\tInvader4_I\tInvader4_LTR\n",
      "Gypsy10\tGypsy10_I\tGypsy10_LTR\n",
      "ACCORD2\tACCORD2_I\tACCORD2_LTR\n",
      "Copia\tCopia_I\tCopia_LTR\n",
      "BEL\tBEL_I\tBEL_LTR\n",
      "Invader5\tInvader5_I\tInvader5_LTR\n",
      "MAX\tMAX_I\tMAX_LTR\n",
      "Gypsy6\tGypsy6_I\tGypsy6_LTR\n",
      "Invader3\tInvader3_I\tInvader3_LTR\n",
      "DM297\tDM297_I\tDM297_LTR\n",
      "Gypsy2\tGypsy2-I_DM\tGypsy2-LTR_DM\n",
      "TIRANT\tTIRANT_I\tTIRANT_LTR\n",
      "Gypsy8\tGypsy8_I\tGypsy8_LTR\n",
      "Copia1\tCopia1-I_DM\tCopia1-LTR_DM\n",
      "ROOA\tROOA_I\tROOA_LTR\n",
      "BLOOD\tBLOOD_I\tBLOOD_LTR\n",
      "Gypsy\tGypsy_I\tGypsy_LTR\n",
      "Gypsy1\tGypsy1-I_DM\tGypsy1-LTR_DM\n",
      "ACCORD\tACCORD_I\tACCORD_LTR\n",
      "Chimpo\tChimpo_I\tChimpo_LTR\n",
      "DIVER2\tDIVER2_I\tDIVER2_LTR\n",
      "DM176\tDM176_I\tDM176_LTR\n",
      "NINJA\tNINJA_I\tNINJA_LTR\n",
      "Gypsy4\tGypsy4_I\tGypsy4_LTR\n",
      "TABOR\tTABOR_I\tTABOR_LTR\n",
      "ROO\tROO_I\tROO_LTR\n",
      "Gypsy11\tGypsy11_I\tGypsy11_LTR\n",
      "TRANSPAC\tTRANSPAC_I\tTRANSPAC_LTR\n",
      "Gypsy12\tGypsy12_I\tGypsy12_LTR\n",
      "FROGGER\tFROGGER_I\tFROGGER_LTR\n",
      "HMSBEAGLE_I\n",
      "TOM_I\n",
      "Stalker3_LTR\n",
      "Gypsy6A_LTR\n",
      "DMTOM1_LTR\n",
      "DM412B_LTR\n",
      "Gypsy12A_LTR\n",
      "XDMR_DM\n",
      "XDMR\n",
      "5S_DM\n",
      "ALA_DM\n",
      "ARS406_DM\n",
      "Baggins1\n",
      "BARI1\n",
      "BARI_DM\n",
      "BS\n",
      "BS2\n",
      "BS3_DM\n",
      "BS4_DM\n",
      "CIRCE\n",
      "DM412\n",
      "DMCR1A\n",
      "DMLTR5\n",
      "DMRP1\n",
      "DMRPR\n",
      "DMRT1A\n",
      "DMRT1B\n",
      "DMRT1C\n",
      "DMSAT6\n",
      "DNAREP1_DM\n",
      "DOC\n",
      "DOC2_DM\n",
      "DOC3_DM\n",
      "DOC4_DM\n",
      "DOC5_DM\n",
      "DOC6_DM\n",
      "FB4_DM\n",
      "FTZ_DM\n",
      "FUSHI_DM\n",
      "FW2_DM\n",
      "FW3_DM\n",
      "G2_DM\n",
      "G3_DM\n",
      "G4_DM\n",
      "G5A_DM\n",
      "G5_DM\n",
      "G6_DM\n",
      "G7_DM\n",
      "G_DM\n",
      "HELENA_RT\n",
      "Helitron1_DM\n",
      "HETA\n",
      "HETRP_DM\n",
      "HOBO\n",
      "I_DM\n",
      "IVK_DM\n",
      "Jockey2\n",
      "LINEJ1_DM\n",
      "LOOPER1_DM\n",
      "M4DM\n",
      "Mariner2_DM\n",
      "MINOS\n",
      "NOF_FB\n",
      "NTS_DM\n",
      "PLACW_DM\n",
      "POGO\n",
      "POGON1\n",
      "PROTOP\n",
      "PROTOP_A\n",
      "PROTOP_B\n",
      "R1-2_DM\n",
      "R1_DM\n",
      "R2_DM\n",
      "RSP\n",
      "SAR2_DM\n",
      "SAR_DM\n",
      "S_DM\n",
      "TAHRE\n",
      "TART-A\n",
      "TART_B1\n",
      "TC1-2_DM\n",
      "TC1_DM\n",
      "TLD2\n",
      "TRANSIB1\n",
      "TRANSIB2\n",
      "TRANSIB3\n",
      "TRANSIB4\n",
      "Transib5\n",
      "Transib-N1_DM\n",
      "S2_DM\n"
     ]
    }
   ],
   "source": [
    "## Dictionnary generator\n",
    "\n",
    "def generate_matching_pairs_from_suffixes(consensus_fasta, suffix_file):\n",
    "    with open(suffix_file, 'r') as input:\n",
    "        I_suffix_list = input.readline().split()\n",
    "        LTR_suffix_list = input.readline().split()\n",
    "    I_suffix_set = set(I_suffix_list)\n",
    "    LTR_suffix_set = set(LTR_suffix_list)\n",
    "\n",
    "    def is_LTR_part(name):\n",
    "        for suffix in LTR_suffix_set:\n",
    "            if name.endswith(suffix):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_I_part(name):\n",
    "        for suffix in I_suffix_set:\n",
    "            if name.endswith(suffix):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def remove_suffix(seq_ID):\n",
    "        suffix_list = list(LTR_suffix_set) + list(I_suffix_set)\n",
    "        for suffix in suffix_list:\n",
    "            if seq_ID.endswith(suffix):\n",
    "                return seq_ID[:-len(suffix)]\n",
    "        return seq_ID\n",
    "\n",
    "    I_part_dict = {}\n",
    "    LTR_part_dict = {}\n",
    "    not_splitted_TE = []\n",
    "\n",
    "    with open(consensus_fasta, 'r') as input:\n",
    "        for line in input:\n",
    "            if line.startswith('>'):\n",
    "                seq_ID = line.split()[-1].replace('>', '')\n",
    "                TE_name = remove_suffix(seq_ID)\n",
    "                if is_LTR_part(seq_ID):\n",
    "                    LTR_part_dict[TE_name] = seq_ID\n",
    "                elif is_I_part(seq_ID):\n",
    "                    I_part_dict[TE_name] = seq_ID\n",
    "                else:\n",
    "                    not_splitted_TE.append(seq_ID)\n",
    "    \n",
    "    matching_TE_parts = set(I_part_dict.keys()).intersection(set(LTR_part_dict.keys()))\n",
    "    unmatched_I_list = [I_part_dict[x] for x in set(I_part_dict.keys()).difference(set(LTR_part_dict.keys()))]\n",
    "    unmatched_LTR_list = [LTR_part_dict[x] for x in set(LTR_part_dict.keys()).difference(set(I_part_dict.keys()))]\n",
    "    if len(unmatched_I_list + unmatched_LTR_list) > 0 :\n",
    "        logging.error(\"These TE names ends with a recognized suffix, but couldn't be matched. You might want to manually cure some of them in the dictionnary :\\n\" + ', '.join(unmatched_I_list) + \"\\n\" + ', '.join(unmatched_LTR_list) + \"\\n\")\n",
    "    TE_dictionnary = \"\"\n",
    "    for TE_name in matching_TE_parts:\n",
    "        TE_dictionnary += \"\\t\".join([TE_name, I_part_dict[TE_name], LTR_part_dict[TE_name]]) + \"\\n\"\n",
    "    TE_dictionnary += '\\n'.join(unmatched_I_list + unmatched_LTR_list + not_splitted_TE)\n",
    "    return TE_dictionnary\n",
    "\n",
    "print(generate_matching_pairs_from_suffixes(Dfam_consensus_fasta, suffix_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating new fasta with flanked intern parts using dictionnary\n",
    "\n",
    "def import_dictionnary(TE_dictionnary):\n",
    "    TE_dict = dict()\n",
    "\n",
    "    with open(TE_dictionnary, 'r') as dictionnary:\n",
    "        for line in dictionnary :\n",
    "            if len(line.split()) == 3:\n",
    "                TE_name, I_part, LTR_part = line.split()\n",
    "                TE_dict[TE_name] = [I_part, LTR_part]\n",
    "\n",
    "            elif len(line.split()) == 1:\n",
    "                TE_dict[TE_name] = []\n",
    "            else :\n",
    "                raise ValueError(\"Incorrect dictionnary format. Make sure every line either contains 3 columns corresponding to the TE_name, TE_intern_part, TE_LTR_part OR only contains 1 column corresponding to the TE_name.\")\n",
    "    return TE_dict\n",
    "\n",
    "print(import_dictionnary(TE_dictionnary))\n",
    "\n",
    "def generating_LTR_flanked_fasta_file(consensus_fasta, TE_dictionnary):\n",
    "    "
   ]
  }
 ]
}